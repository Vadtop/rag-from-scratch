"""
RAG Evaluation Tool
Measures: faithfulness and relevancy
"""

import json
import os
from dotenv import load_dotenv
import requests

load_dotenv()

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")

def load_test_questions(filepath="test_questions.json"):
    """Load test dataset"""
    with open(filepath, "r", encoding="utf-8") as f:
        return json.load(f)

def rag_query(question):
    """
    Simplified RAG query - uses LangChain API endpoint
    Returns answer + retrieved chunks
    """
    # –í—ã–∑—ã–≤–∞–µ–º LangChain endpoint
    response = requests.post(
        "http://localhost:8000/query_langchain",
        json={"query": question, "top_k": 3}
    )
    data = response.json()
    return data.get("answer", ""), data.get("sources", [])

def calculate_faithfulness(answer: str, sources: list) -> float:
    """
    Faithfulness: simplified check - if answer is not empty and sources exist
    In production: use LLM-as-judge or advanced metrics
    """
    if not sources or not answer or answer.lower().startswith("i don't know"):
        return 0.0
    
    # Simple heuristic: if answer contains substantial content (>30 chars)
    # and sources exist, assume faithful
    return 1.0 if len(answer) > 30 else 0.0
    
    # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—å –æ–¥–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    answer_lower = answer.lower()
    mentioned = sum(1 for src in sources if any(word in answer_lower for word in src.lower().split()))
    
    return min(mentioned / len(sources), 1.0)

def calculate_relevancy(sources: list, expected_sources: list) -> float:
    """
    Relevancy: –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–∞—à–ª–∏—Å—å –ª–∏ –Ω—É–∂–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    """
    if not expected_sources:
        return 1.0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–∫–æ–ª—å–∫–æ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –Ω–∞—à–ª–æ—Å—å
    sources_str = " ".join(sources).lower()
    found = sum(1 for expected in expected_sources if expected.lower() in sources_str)
    
    return found / len(expected_sources)

def evaluate_rag():
    """
    –ü—Ä–æ–≥–æ–Ω—è–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏ —Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏
    """
    print("‚ö†Ô∏è  –ó–∞–ø—É—Å—Ç–∏ API –ø–µ—Ä–µ–¥ –æ—Ü–µ–Ω–∫–æ–π: uvicorn api:app --reload")
    input("–ù–∞–∂–º–∏ Enter –∫–æ–≥–¥–∞ API –∑–∞–ø—É—â–µ–Ω...")
    
    questions = load_test_questions()
    
    results = []
    total_faithfulness = 0
    total_relevancy = 0
    
    print("\n" + "="*60)
    print("üîç RAG EVALUATION")
    print("="*60)
    
    for i, item in enumerate(questions, 1):
        question = item["question"]
        expected_sources = item.get("expected_sources", [])
        difficulty = item.get("difficulty", "medium")
        
        print(f"\n[{i}/{len(questions)}] ({difficulty.upper()}) {question}")
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç RAG
            answer, sources = rag_query(question)
            
            # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            faithfulness = calculate_faithfulness(answer, sources)
            relevancy = calculate_relevancy(sources, expected_sources)
            
            total_faithfulness += faithfulness
            total_relevancy += relevancy
            
            results.append({
                "question": question,
                "difficulty": difficulty,
                "answer": answer[:80] + "...",
                "faithfulness": faithfulness,
                "relevancy": relevancy,
                "sources": sources
            })
            
            print(f"  ‚úÖ Faithfulness: {faithfulness:.2f}")
            print(f"  ‚úÖ Relevancy: {relevancy:.2f}")
            print(f"  üìÑ Sources: {', '.join(sources[:2])}")
            
        except Exception as e:
            print(f"  ‚ùå Error: {str(e)}")
            continue
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–æ—Å—Ç–∞–ª—å–Ω–æ–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    # ...
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    avg_faithfulness = total_faithfulness / len(questions)
    avg_relevancy = total_relevancy / len(questions)
    
    print("\n" + "="*60)
    print("üìä OVERALL METRICS")
    print("="*60)
    print(f"Average Faithfulness: {avg_faithfulness:.2%}")
    print(f"Average Relevancy: {avg_relevancy:.2%}")
    print(f"Overall Score: {(avg_faithfulness + avg_relevancy) / 2:.2%}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "results": results,
            "metrics": {
                "faithfulness": avg_faithfulness,
                "relevancy": avg_relevancy,
                "overall": (avg_faithfulness + avg_relevancy) / 2
            }
        }, f, indent=2, ensure_ascii=False)
    
    print("\nüíæ Results saved to evaluation_results.json")

if __name__ == "__main__":
    evaluate_rag()
