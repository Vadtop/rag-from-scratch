# step2_langchain.py
"""
LangChain-based RAG implementation for comparison with manual approach.
"""

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
from dotenv import load_dotenv

load_dotenv()

# ========== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==========
API_KEY = os.environ["OPENROUTER_API_KEY"]
BASE_URL = "https://openrouter.ai/api/v1"
PERSIST_DIR = "./chroma_langchain_db"

# ========== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ==========

class LangChainRAG:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LangChain RAG —Å–∏—Å—Ç–µ–º—ã"""
        
        # Embeddings
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=API_KEY,
            openai_api_base=BASE_URL
        )
        
        # LLM
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0,
            openai_api_key=API_KEY,
            openai_api_base=BASE_URL
        )
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π vectorstore
        try:
            self.vectorstore = Chroma(
                persist_directory=PERSIST_DIR,
                embedding_function=self.embeddings
            )
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π vectorstore ({self.vectorstore._collection.count()} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
        except:
            self.vectorstore = None
            print("‚ö†Ô∏è Vectorstore –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load_documents() –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è.")
        
        # QA chain
        if self.vectorstore:
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3})
            )
    
    def load_documents(self, directory="documents/"):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞—Ç—å vectorstore"""
        
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ {directory}...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞
        loader = DirectoryLoader(
            directory,
            glob="*.txt",
            loader_cls=TextLoader
        )
        documents = loader.load()
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
        
        # Chunking (–∫–∞–∫ –≤ manual RAG –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100
        )
        chunks = text_splitter.split_documents(documents)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ chunks: {len(chunks)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ vectorstore
        print("‚è≥ –°–æ–∑–¥–∞—é embeddings...")
        self.vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=self.embeddings,
            persist_directory=PERSIST_DIR
        )
        print("‚úÖ Vector store –≥–æ—Ç–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–∞ –¥–∏—Å–∫!")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ QA chain
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3})
        )
        
        return len(chunks)
    
    def query(self, question: str) -> dict:
        """–ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å RAG —Å–∏—Å—Ç–µ–º–µ"""
        
        if not self.qa_chain:
            return {
                "answer": "Vectorstore not initialized. Please load documents first.",
                "sources": []
            }
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        answer = self.qa_chain.run(question)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        docs = self.vectorstore.similarity_search(question, k=3)
        sources = list(set([doc.metadata.get('source', 'unknown') for doc in docs]))
        
        return {
            "answer": answer,
            "sources": sources,
            "chunks_used": len(docs)
        }


# ========== –¢–ï–°–¢–´ (–µ—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—à—å –Ω–∞–ø—Ä—è–º—É—é) ==========

if __name__ == "__main__":
    print("="*60)
    print("ü§ñ LANGCHAIN RAG –°–ò–°–¢–ï–ú–ê")
    print("="*60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    rag = LangChainRAG()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã)
    if rag.vectorstore is None or rag.vectorstore._collection.count() == 0:
        rag.load_documents()
    
    # –¢–µ—Å—Ç—ã
    print("\n" + "="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï")
    print("="*60)
    
    questions = [
        "How does RAG work?",
        "What is Python used for?",
        "What are vector databases?"
    ]
    
    for q in questions:
        print(f"\nüìù –í–æ–ø—Ä–æ—Å: {q}")
        result = rag.query(q)
        print(f"‚úÖ –û–¢–í–ï–¢:\n{result['answer']}")
        print(f"üìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {', '.join(result['sources'])}")
        print("-"*60)
